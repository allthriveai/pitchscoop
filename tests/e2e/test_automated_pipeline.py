#!/usr/bin/env python3
"""Test complete automated Recording -> Scoring -> Leaderboard pipeline."""

import asyncio
import json
import pytest

@pytest.mark.asyncio
async def test_full_automation():
    """Test the complete automated pipeline."""
    
    print("üöÄ TESTING COMPLETE AUTOMATED PIPELINE")
    print("=" * 60)
    print("Recording ‚Üí Automatic Scoring ‚Üí Leaderboard")
    print()
    
    try:
        # Step 1: Create a test recording session (simulates recording completion)
        print("1. üìπ Simulating Recording Completion:")
        print("-" * 40)
        
        import redis.asyncio as redis
        client = redis.from_url('redis://redis:6379/0', decode_responses=True)
        
        # Create a new test session with a different ID to test automation
        test_session = {
            'session_id': 'automated-test-session-002',
            'event_id': 'automation-test-2024',
            'team_name': 'AutoBot AI',
            'pitch_title': 'Automated Customer Service Agent',
            'status': 'completed',
            'created_at': '2024-01-15T10:00:00Z',
            'completed_at': '2024-01-15T10:04:00Z',
            'final_transcript': {
                'total_text': ("Hello judges, we are AutoBot AI, creators of an intelligent customer service agent. "
                              "Our AI agent uses natural language processing to handle customer inquiries 24/7. "
                              "We integrate four key sponsor tools: OpenAI for natural language understanding, "
                              "Qdrant for knowledge base search and retrieval, MinIO for secure document storage, "
                              "and LangChain for multi-step conversation flows. Our agent can resolve 85% of "
                              "customer inquiries without human intervention, reducing response time from hours "
                              "to seconds. We've processed over 50,000 customer interactions with a 94% "
                              "satisfaction rate. The agent learns from each interaction, continuously improving "
                              "its responses. Thank you."),
                'segments_count': 8,
                'segments': []
            }
        }
        
        # Store the test session
        session_key = 'event:automation-test-2024:session:automated-test-session-002'
        await client.setex(session_key, 3600, json.dumps(test_session))
        
        print(f"‚úÖ Created test session:")
        print(f"   Team: {test_session['team_name']}")
        print(f"   Pitch: {test_session['pitch_title']}")
        print(f"   Transcript: {len(test_session['final_transcript']['total_text'])} chars")
        
        # Step 2: Trigger automatic scoring (this simulates the automatic trigger)
        print()
        print("2. üß† Testing Automatic Scoring:")
        print("-" * 40)
        
        from api.domains.recordings.mcp.gladia_mcp_handler import gladia_mcp_handler
        
        # Test the automatic scoring trigger method directly
        class MockLogger:
            def info(self, msg, **kwargs):
                print(f"[INFO] {msg}")
            def error(self, msg, **kwargs):
                print(f"[ERROR] {msg}")
            def warning(self, msg, **kwargs):
                print(f"[WARN] {msg}")
        
        mock_logger = MockLogger()
        
        await gladia_mcp_handler._trigger_automatic_scoring(
            session_id='automated-test-session-002',
            event_id='automation-test-2024',
            team_name='AutoBot AI',
            logger=mock_logger
        )
        
        # Step 3: Verify scoring was created
        print()
        print("3. ‚úÖ Verifying Automatic Scoring:")
        print("-" * 40)
        
        scoring_key = 'event:automation-test-2024:scoring:automated-test-session-002'
        scoring_data = await client.get(scoring_key)
        
        if scoring_data:
            data = json.loads(scoring_data)
            analysis = data.get('analysis', {})
            
            if 'overall' in analysis:
                total_score = analysis['overall'].get('total_score', 0)
                print(f"‚úÖ Automatic scoring successful!")
                print(f"   Team: {data.get('team_name')}")
                print(f"   Total Score: {total_score}")
                print(f"   Scoring Method: {data.get('scoring_method')}")
                
                # Show trigger context
                context = data.get('scoring_context', {})
                if context.get('triggered_by'):
                    print(f"   Triggered By: {context['triggered_by']}")
                    
            else:
                print("‚ùå Scoring data malformed")
        else:
            print("‚ùå No automatic scoring data found")
        
        # Step 4: Test leaderboard with automated data
        print()
        print("4. üèÜ Testing Leaderboard with Automated Data:")
        print("-" * 40)
        
        from api.domains.leaderboards.mcp.leaderboard_mcp_tools import execute_leaderboard_mcp_tool
        
        result = await execute_leaderboard_mcp_tool('leaderboard.get_rankings', {
            'event_id': 'automation-test-2024',
            'limit': 10
        })
        
        if result.get('success'):
            leaderboard = result.get('leaderboard', [])
            print(f"‚úÖ Leaderboard generated with {len(leaderboard)} teams")
            
            for entry in leaderboard:
                rank = entry.get('rank')
                name = entry.get('team_name')
                score = entry.get('total_score')
                print(f"   #{rank}: {name} - {score:.1f} pts")
        else:
            print(f"‚ùå Leaderboard failed: {result.get('error')}")
        
        await client.aclose()
        
        print()
        print("üéâ FULL AUTOMATION TEST COMPLETE!")
        print("‚úÖ Recording ‚Üí Automatic Scoring ‚Üí Leaderboard pipeline working!")
        
    except Exception as e:
        print(f"‚ùå Test failed: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_full_automation())